---
phase: 03-transcription-api
plan: 02
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - DictationApp/Sources/Services/TranscriptionManager.swift
  - DictationApp/Sources/Services/HotkeyManager.swift
autonomous: false

must_haves:
  truths:
    - "Recording completion triggers transcription automatically"
    - "English speech is transcribed accurately"
    - "German speech is transcribed accurately"
    - "Transcription result is logged (ready for Phase 4 paste)"
  artifacts:
    - path: "DictationApp/Sources/Services/TranscriptionManager.swift"
      provides: "Transcription orchestration"
      min_lines: 40
    - path: "DictationApp/Sources/Services/HotkeyManager.swift"
      provides: "TranscriptionManager integration"
      contains: "TranscriptionManager"
  key_links:
    - from: "HotkeyManager.handleHotkeyPressed"
      to: "TranscriptionManager.handleRecordingCompletion"
      via: "async call after stopRecording"
      pattern: "TranscriptionManager.*handleRecordingCompletion"
    - from: "TranscriptionManager"
      to: "APIClient.transcribe"
      via: "async transcription call"
      pattern: "APIClient.shared.transcribe"
    - from: "TranscriptionManager"
      to: "UserDefaults"
      via: "language preference lookup"
      pattern: "UserDefaults.*transcriptionLanguage"
---

<objective>
Integrate transcription workflow with recording completion

Purpose: Wire up the transcription service to automatically process recorded audio when the user stops recording. The transcription result will be logged and made available for Phase 4 paste functionality.

Output: TranscriptionManager service that handles the recording-to-transcription flow, integrated with HotkeyManager.
</objective>

<execution_context>
@/Users/justindeisler/.claude/get-shit-done/workflows/execute-plan.md
@/Users/justindeisler/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-transcription-api/03-RESEARCH.md
@.planning/phases/03-transcription-api/03-01-SUMMARY.md

# Source files to integrate
@DictationApp/Sources/Services/HotkeyManager.swift
@DictationApp/Sources/Services/APIClient.swift
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create TranscriptionManager and integrate with HotkeyManager</name>
  <files>
    DictationApp/Sources/Services/TranscriptionManager.swift
    DictationApp/Sources/Services/HotkeyManager.swift
  </files>
  <action>
1. Create `DictationApp/Sources/Services/TranscriptionManager.swift`:
   ```swift
   import Foundation

   /// Notification for transcription completion (ready for Phase 4 paste)
   extension Notification.Name {
       static let transcriptionDidComplete = Notification.Name("transcriptionDidComplete")
       static let transcriptionDidFail = Notification.Name("transcriptionDidFail")
   }

   /// Manages transcription workflow: audio file -> Groq API -> text result
   @MainActor
   final class TranscriptionManager {
       static let shared = TranscriptionManager()

       private init() {}

       /// Process a recorded audio file and transcribe it
       /// - Parameter audioURL: URL to the recorded WAV file
       /// - Returns: Transcribed text on success
       @discardableResult
       func handleRecordingCompletion(audioURL: URL) async -> String? {
           // Get language preference from UserDefaults
           let languagePreference = UserDefaults.standard.string(forKey: "transcriptionLanguage") ?? "auto"
           let language: String? = languagePreference == "auto" ? nil : languagePreference

           do {
               print("Starting transcription for: \(audioURL.lastPathComponent)")
               let result = try await APIClient.shared.transcribe(audioURL: audioURL, language: language)
               print("Transcription complete: \(result.text)")

               // Post notification for Phase 4 paste integration
               NotificationCenter.default.post(
                   name: .transcriptionDidComplete,
                   object: result.text
               )

               return result.text
           } catch let error as APIError {
               print("Transcription failed: \(error.userMessage)")
               NotificationCenter.default.post(
                   name: .transcriptionDidFail,
                   object: error.userMessage
               )
               return nil
           } catch {
               print("Transcription failed: \(error.localizedDescription)")
               NotificationCenter.default.post(
                   name: .transcriptionDidFail,
                   object: error.localizedDescription
               )
               return nil
           }
       }
   }
   ```

2. Update `HotkeyManager.swift` to call TranscriptionManager after recording stops:

   In `handleHotkeyPressed()`, replace the stop recording block:
   ```swift
   // Stop recording (REC-02)
   if let recordingURL = recorder.stopRecording() {
       print("Recording stopped. File saved to: \(recordingURL.path)")
       NotificationCenter.default.post(
           name: .recordingDidStop,
           object: recordingURL
       )

       // Trigger transcription (TRX-01)
       Task {
           await TranscriptionManager.shared.handleRecordingCompletion(audioURL: recordingURL)
       }
   }
   ```

3. Add TranscriptionManager.swift to Xcode project:
   - Open project in Xcode or use PBXProj manipulation
   - Add file to Services group
  </action>
  <verify>
  Build succeeds and integration is correct:
  ```bash
  cd /Users/justindeisler/Desktop/Developing/Projects/DictationApp && xcodebuild -project DictationApp/DictationApp.xcodeproj -scheme DictationApp -configuration Debug build 2>&1 | tail -20
  grep -n "TranscriptionManager" DictationApp/Sources/Services/HotkeyManager.swift
  grep -n "handleRecordingCompletion" DictationApp/Sources/Services/TranscriptionManager.swift
  ```
  </verify>
  <done>
  - TranscriptionManager.swift exists with handleRecordingCompletion method
  - HotkeyManager calls TranscriptionManager after stopRecording
  - Notifications defined for transcriptionDidComplete and transcriptionDidFail
  - Build succeeds
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
    Complete Phase 3 transcription workflow:
    - APIClient with Groq Whisper API integration (whisper-large-v3-turbo model)
    - Language preference in Settings (Auto-detect, English, German)
    - TranscriptionManager that processes recordings automatically
    - End-to-end: Option+Space to record, release to transcribe
  </what-built>
  <how-to-verify>
    1. Build and run the app:
       ```bash
       cd /Users/justindeisler/Desktop/Developing/Projects/DictationApp
       xcodebuild -project DictationApp/DictationApp.xcodeproj -scheme DictationApp -configuration Debug build
       open DictationApp/DictationApp.xcodeproj
       # Run with Cmd+R in Xcode
       ```

    2. Verify Settings language picker:
       - Click menu bar icon → Settings
       - Confirm "Transcription" section exists with language picker
       - Change to "English" and close window
       - Reopen Settings - verify "English" is still selected (persisted)

    3. Test English transcription (TRX-03):
       - Ensure API key is configured in Settings
       - Press Option+Space to start recording
       - Speak clearly in English: "This is a test of the dictation application"
       - Press Option+Space to stop
       - Check Xcode console for: "Transcription complete: [your text]"
       - Verify the text matches what you said

    4. Test German transcription (TRX-04):
       - Open Settings, change language to "German"
       - Press Option+Space to start recording
       - Speak in German: "Das ist ein Test der Diktierfunktion"
       - Press Option+Space to stop
       - Check Xcode console for transcription result
       - Verify German text is transcribed accurately

    5. Test auto-detect:
       - Open Settings, change language to "Auto-detect"
       - Test with both English and German phrases
       - Verify both are transcribed (may be slightly less accurate than manual language selection)

    6. Expected console output pattern:
       ```
       Recording started...
       Recording stopped. File saved to: /var/folders/.../audio_XXXXXX.wav
       Starting transcription for: audio_XXXXXX.wav
       Transcription complete: [transcribed text here]
       ```

    Note: If transcription fails with "invalid API key", verify API key is saved in Settings.
    If no transcription output appears, check Xcode console for error messages.
  </how-to-verify>
  <resume-signal>
    Type "approved" if transcription works for both English and German.
    If issues, describe what's happening (e.g., "transcription returns empty", "timeout error", "no console output").
  </resume-signal>
</task>

</tasks>

<verification>
Phase 3 requirements coverage:
- TRX-01: Audio sent to Groq API after recording stops ✓ (TranscriptionManager.handleRecordingCompletion)
- TRX-02: Uses whisper-large-v3-turbo model ✓ (hardcoded in APIClient.transcribe)
- TRX-03: English transcription ✓ (verified in checkpoint)
- TRX-04: German transcription ✓ (verified in checkpoint)
- TRX-05: Language configurable in settings ✓ (SettingsView picker)

```bash
# Verify all Phase 3 components
grep -l "whisper-large-v3-turbo" DictationApp/Sources/Services/APIClient.swift
grep -l "transcriptionLanguage" DictationApp/Sources/Views/SettingsView.swift
grep -l "TranscriptionManager" DictationApp/Sources/Services/HotkeyManager.swift
```
</verification>

<success_criteria>
- [ ] Project builds successfully
- [ ] TranscriptionManager exists and handles recording completion
- [ ] HotkeyManager integrates TranscriptionManager
- [ ] English speech is transcribed accurately
- [ ] German speech is transcribed accurately
- [ ] Language preference is respected
- [ ] Console shows transcription results
- [ ] All 5 TRX requirements covered
</success_criteria>

<output>
After completion, create `.planning/phases/03-transcription-api/03-02-SUMMARY.md`
</output>
