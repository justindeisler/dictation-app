---
phase: 03-transcription-api
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - DictationApp/Sources/Models/TranscriptionResult.swift
  - DictationApp/Sources/Services/APIClient.swift
  - DictationApp/Sources/Views/SettingsView.swift
autonomous: true

must_haves:
  truths:
    - "APIClient can send WAV files to Groq Whisper API"
    - "Transcription returns text from audio"
    - "User can configure language preference in settings"
  artifacts:
    - path: "DictationApp/Sources/Models/TranscriptionResult.swift"
      provides: "Transcription response model"
      min_lines: 10
    - path: "DictationApp/Sources/Services/APIClient.swift"
      provides: "Transcription method with multipart upload"
      contains: "func transcribe"
    - path: "DictationApp/Sources/Views/SettingsView.swift"
      provides: "Language preference picker"
      contains: "transcriptionLanguage"
  key_links:
    - from: "APIClient.transcribe"
      to: "https://api.groq.com/openai/v1/audio/transcriptions"
      via: "URLSession multipart POST"
      pattern: "audio/transcriptions"
    - from: "SettingsView"
      to: "UserDefaults"
      via: "@AppStorage"
      pattern: "AppStorage.*transcriptionLanguage"
---

<objective>
Implement Groq Whisper API transcription capability and language settings

Purpose: Enable the app to transcribe recorded WAV audio files via Groq's Whisper API and allow users to configure language preference (English, German, or auto-detect).

Output: Extended APIClient with transcription method, TranscriptionResult model, and language picker in SettingsView.
</objective>

<execution_context>
@/Users/justindeisler/.claude/get-shit-done/workflows/execute-plan.md
@/Users/justindeisler/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-transcription-api/03-RESEARCH.md

# Prior plan summaries - needed for established patterns
@.planning/phases/01-foundation-settings/01-02-SUMMARY.md

# Source files to extend
@DictationApp/Sources/Services/APIClient.swift
@DictationApp/Sources/Views/SettingsView.swift
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add TranscriptionResult model and extend APIClient with transcription method</name>
  <files>
    DictationApp/Sources/Models/TranscriptionResult.swift
    DictationApp/Sources/Services/APIClient.swift
  </files>
  <action>
1. Create `DictationApp/Sources/Models/` directory if needed and add `TranscriptionResult.swift`:
   - Struct with `text: String` property
   - Conform to `Codable, Sendable` for Swift 6 concurrency

2. Add `fileTooLarge` case to `APIError` enum in APIClient.swift:
   ```swift
   case fileTooLarge(size: Int64, limit: Int64)
   ```
   Add corresponding `userMessage` for this case.

3. Add a separate URLSession property for transcription with 60-second timeout:
   ```swift
   private let transcriptionSession: URLSession
   ```
   Initialize in `init()` with `timeoutIntervalForRequest: 60` and `timeoutIntervalForResource: 60`.

4. Add `Data` extension for multipart body construction:
   ```swift
   private extension Data {
       mutating func append(_ string: String) {
           if let data = string.data(using: .utf8) {
               append(data)
           }
       }
   }
   ```

5. Add `transcribe(audioURL:language:)` method to APIClient:
   - Validate file exists and size <= 25MB (free tier limit)
   - Load API key from KeychainManager.shared
   - Create multipart/form-data request with:
     - `file`: The audio data
     - `model`: "whisper-large-v3-turbo" (TRX-02)
     - `language`: Optional, passed through when not nil
   - Use UUID().uuidString for boundary (prevent collisions)
   - POST to `{baseURL}/audio/transcriptions`
   - Parse response using TranscriptionResult
   - Handle error cases: 401 (invalidAPIKey), 429 (rateLimitExceeded), timeout, network errors

Reference Pattern: See APIClient.validateAPIKey for error handling structure. Use same switch-case pattern for HTTP status codes.
  </action>
  <verify>
  Build succeeds:
  ```bash
  cd /Users/justindeisler/Desktop/Developing/Projects/DictationApp && xcodebuild -project DictationApp/DictationApp.xcodeproj -scheme DictationApp -configuration Debug build 2>&1 | tail -20
  ```
  </verify>
  <done>
  - TranscriptionResult.swift exists with Codable+Sendable struct
  - APIClient has `transcribe(audioURL:language:)` method
  - APIError has fileTooLarge case
  - Build succeeds with no errors
  </done>
</task>

<task type="auto">
  <name>Task 2: Add language preference to SettingsView</name>
  <files>
    DictationApp/Sources/Views/SettingsView.swift
  </files>
  <action>
1. Add @AppStorage property for language preference:
   ```swift
   @AppStorage("transcriptionLanguage") private var languagePreference: String = "auto"
   ```

2. Add a new Section in the Form after the API Configuration section:
   ```swift
   Section {
       Picker("Language", selection: $languagePreference) {
           Text("Auto-detect").tag("auto")
           Text("English").tag("en")
           Text("German").tag("de")
       }
       .pickerStyle(.segmented)

       Text("Specifying a language improves accuracy and speed")
           .font(.caption)
           .foregroundStyle(.secondary)
   } header: {
       Text("Transcription")
           .font(.headline)
   }
   ```

3. Increase window height to accommodate new section:
   - Change `.frame(width: 500, height: 250)` to `.frame(width: 500, height: 320)`
   - Update `window.setContentSize(NSSize(width: 500, height: 320))` in AppDelegate.swift

Note: Language setting auto-saves via @AppStorage, no Save button interaction needed. This follows Apple's standard Settings pattern where non-destructive preferences apply immediately.
  </action>
  <verify>
  Build succeeds and window size updated:
  ```bash
  cd /Users/justindeisler/Desktop/Developing/Projects/DictationApp && xcodebuild -project DictationApp/DictationApp.xcodeproj -scheme DictationApp -configuration Debug build 2>&1 | tail -20
  grep -n "height: 320" DictationApp/Sources/Views/SettingsView.swift
  grep -n "height: 320" DictationApp/Sources/App/AppDelegate.swift
  ```
  </verify>
  <done>
  - SettingsView has language Picker with Auto-detect/English/German options
  - @AppStorage persists selection to UserDefaults
  - Window height increased to 320 to fit new section
  - Build succeeds
  </done>
</task>

</tasks>

<verification>
After both tasks complete:
1. Build succeeds with no warnings related to new code
2. `TranscriptionResult.swift` exists in Models directory
3. `APIClient.swift` contains `transcribe` method with multipart/form-data handling
4. `SettingsView.swift` contains language picker with "transcriptionLanguage" key
5. AppDelegate has updated window size

```bash
# Verify all files exist and contain expected content
grep -l "transcribe" DictationApp/Sources/Services/APIClient.swift
grep -l "TranscriptionResult" DictationApp/Sources/Models/TranscriptionResult.swift
grep -l "transcriptionLanguage" DictationApp/Sources/Views/SettingsView.swift
```
</verification>

<success_criteria>
- [ ] Project builds successfully
- [ ] APIClient.transcribe method exists with correct signature
- [ ] TranscriptionResult model is Codable and Sendable
- [ ] Language picker in SettingsView with 3 options (auto, en, de)
- [ ] Window height updated in both SettingsView and AppDelegate
- [ ] Multipart boundary uses UUID for collision resistance
- [ ] File size validation checks <= 25MB before upload
- [ ] 60-second timeout configured for transcription requests
</success_criteria>

<output>
After completion, create `.planning/phases/03-transcription-api/03-01-SUMMARY.md`
</output>
